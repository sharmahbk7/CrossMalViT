max_epochs: 100
batch_size: 32
num_workers: 8
learning_rate: 0.00012
weight_decay: 0.048

ldam_margin: 0.48
ldam_scale: 30.0
lambda_contrast: 0.32
temperature: 0.07

mixup_alpha: 0.42
cutmix_prob: 0.52

mixed_precision: true
gradient_clip: 1.0
accumulate_grad: 1
patience: 15
